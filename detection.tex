\chapter{Detecting Completed Steps of Complex Tasks}\label{chap:detection}

A WCA application for a physical assembly task must detect when a step of the
task has been completed.
A user wears a mobile device with a camera (such as a Google Glass).
The mobile
device gives the user an instruction, waits for them to complete this, and then
gives them the next instruction.
Completing a step might require adding a part to the assembly,
removing a part from the assembly, or repositioning the object in order to give
the camera a certain view.
The application must determine when a step is completed
based on images from the camera.
It is important to avoid false positives, which are instances where the
application determines that a step has been completed before it actually has
been.
False positives cause the application to give the user a new instruction before
the previous step gets completed, which results in a poor user experience.
When a false negative occurs, the application will not give the user a new
instruction after they have completed a step.
However, false negatives can typically be corrected by slightly rotating the
assembly, which will give the camera a different view of the object and thus get
the classifier to assign a correct label.
False negatives result in a suboptimal user experience, but they
are less disorienting than false positives.

Developers must train computer vision models to recognize when each step of the
task has been completed.
They must also train the models for each error state that they want the
application to be able to recognize.
Unfortunately, there is a combinatorial explosion in the number of possible
error states, which we address in Chapter~\ref{chap:escalation}.

This chapter presents the approaches we have developed to detect the step of a
physical task that is shown in an image, in the context of WCA.
We wanted to develop something that takes an image captured by a headset and
determines the step of the task that is shown in that image.
If a user has made a mistake that the application was developed to recognize, we
want the application to be able to recognize this as well.

\section{Hierarchical Decomposition}

Inspired by Simon's argument that
all complex systems are made up of smaller systems~\cite{Simon1991}, we argue
that any object that is assembled from more than ten parts can be decomposed
into sub-assemblies.
The CAD programs Autodesk Inventor and Intergraph Smart 3D both represent
assemblies as a
hierarchy~\cite{autodesk_hierarchy, intergraph_hierarchy}.
\citet{semantic_hierarchy} utilized hierarchies of visual features to recognize
objects.
\citet{subassembly_identification} automatically identified subassemblies of
assembled objects based on CAD files.
They utilize features such as the number of other parts that a certain part
touches, or the amount of surface area that two touching parts share.

The lower bound of ten parts comes from our firsthand experience developing WCA
applications.
We have successfully developed applications for objects assembled out of ten or
fewer parts without splitting the task into subassemblies.
However, for objects assembled using more than ten parts, splitting the task
into subassemblies has proven to be a useful technique.

This technique is applicable to tasks
with multiple levels of sub-assembly hierarchies.
An assistant for a task involving multiple sub-assemblies is effectively a
series of independent applications. Once the user completes one sub-assembly,
they will automatically be taken to the assistant for the next one.
If the
sub-assemblies must be connected together after that, there will be an assistant
for these steps as well.
Tasks can be broken into sub-assemblies the same way that long documents can be
broken into chapters, sections, and subsections.
Sub-assemblies near the top of the hierarchy are going to be made up of multiple
levels of sub-assemblies below them.
Hierarchical Decomposition can be used to split a task that requires hundreds of
steps, or even thousands of steps, into sub-assemblies that can be assembled in
10 steps or fewer.
Our initial WCA applications were all for 10 step tasks.
It thus made sense to develop WCA applications for longer tasks as a series of
tasks that required 10 or fewer steps.

Figure~\ref{fig:stirling_full} shows two of the sub-assemblies of a Stirling
engine.
Our application uses a different pair of computer vision models for each
sub-assembly.
The code selects the correct pair based on the current step that the user is
working on.
Each step involves attaching a piece to one sub-assembly.
None of the sub-assemblies involve more than 8 steps.
Splitting the task up into sub-assemblies thus simplifies the scope of the
problem to developing a set of assistants for 8 step tasks.

The number of steps required for each sub-assembly is not something that we have
any fixed rules about.
The optimal number of steps for a sub-assembly may vary based on the task.
However, limiting the number of steps to 10 worked well for all of the
applications that we have developed.

\begin{figure}
  \includegraphics[width=7cm]{figures/stirling/full.png}
  \caption{A stirling engine with two sub-assemblies highlighted
  }\label{fig:stirling_full}
\end{figure}

Figure~\ref{fig:erector} shows how a model motorcycle can be broken up into
three sub-assemblies.
The Stirling engine has a single large base, but the motorcycle is simply
assembled from small pieces. Therefore, assembling the motorcycle requires
an additional set of steps at the end, to connect the sub-assemblies together.

\begin{figure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=5cm]{figures/erector/full.jpg}
    \caption{The fully-assembled model}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=5cm]{figures/erector/sub1.jpg}
    \includegraphics[width=5cm]{figures/erector/sub2.jpg}
    \includegraphics[width=5cm]{figures/erector/sub3.jpg}
    \caption{Sub-assemblies}
  \end{subfigure}
  \caption{A model motorcycle from a Meccano Erector kit.
    The sub-assemblies are first assembled from smaller components.
    Next, the user combines the sub-assemblies together into the fully-assembled
    motorcycle.
    The application uses a fine-grained image classifier and an object detector
    for the final steps of putting the 3 sub-assemblies together.
    In these final steps, each of the 3 sub-assemblies is treated like a part
    that the user is attaching to the final object being assembled.
  }\label{fig:erector}
\end{figure}

\section{Two Stage Process}\label{sec:two_stage}

An assembly task involves attaching components to form an object.
The object is made up of the components that are assembled together, so it must
be larger in size than any of the individual components.
A part that gets attached in one step of the task might be a lot smaller than
the whole object.
For example, one step might ask the user to insert a single screw into a large
metal engine.
Having the user move their head close to the part they just attached, in order
to give the camera a better view of how the part was attached, would make it
easier for computer vision models to determine when a step has been computed.
However, this would be cumbersome for a user.

Instead, the application should work while a user keeps their head in a position
that feels natural for completing a task.
This requires the system to determine when a step has been
completed based on an image that contains most or all of the full object being
assembled.
We accomplish this using a two stage process, where the system first finds the
region of an image that contains the sub-assembly involved in a step.
It then
crops the image around this region, and the next model determines if the step
has been completed based on the cropped image.

This process is similar to the two stage process used by
\citet{gebru2017finegrained}.
The first stage involves finding the region of the
image that contains the subassembly that a user is working on, using Faster
R-CNN~\cite{frcnn}.
Next, the image is cropped around this region, and the cropped region is
classified using Fast MPN-COV~\cite{Li_2018_CVPR}.
There is one Fast MPN-COV model per subassembly.
The Fast MPN-COV model has one label for each step of the task that is part of
this model's subassembly.
The classification result therefore indicates the step of the task that is shown
in an image.
The application considers a step to be complete when an image from the camera
feed is classified as the label corresponding to the next step.
The architecture for our applications is shown in Figure~\ref{fig:arch}.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/architecture.pdf}
  \caption{
    The architecture of our WCA applications.
    The dashed lines represent a Wi-Fi connection.
    The solid lines represent a connection over Gigabit Lan.
    The dotted lines represent data transmission between components on a single
    cloudlet.
  }\label{fig:arch}
\end{figure}

\subsection{Re-Using Labels}

A single label may correspond to multiple steps of a task.
For example, a kit might contain two identical subassemblies that get assembled
on their own, before being connected to the rest of the kit.
The steps required to assemble both of these subassemblies will be identical.
The subassemblies do not get connected to the rest of the kit until after they
have been assembled, so there will not be any visible differences while the user
is assembling one or the other.
The application can therefore use the same sequence of outputs from a computer
vision model for both subassemblies.
However, two consecutive steps cannot share the same label.
The application considers a step to be completed when images are classified with
the label corresponding to the next step.
If the next step had the same label, the application would think that the user
completed a step immediately after the step was started.
For example, imagine that a developer trained a fine-grained image classifier
that had a label it used when two black screws were inserted in an assembly.
If this developer created an application that had two consecutive steps that
would be completed when the classifier outputs ``two black screws,'' the
application would immediately advance past the second of these steps.
The classifier must have already outputted the ``two black screws'' label in
order for the application to advance past the first ``two black screws'' step.
The model will continue to output ``two black screws'' unless the subassembly is
modified.
The application will thus immediately advance past the second
``two black screws'' step.
This issue is illustrated in Figure~\ref{fig:consec_step}.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/consec_step.pdf}
  \caption{
    The sequence of steps depicted in the top row cannot be supported by our
    techniques because the two consecutive steps are identical.
    There is no visually apparent difference between the two images in the top
    row.
    The sequence of steps depicted in the bottom row is acceptable because the
    identical steps are not consecutive.
    The image in the middle is different from the images on the right and left.
  }\label{fig:consec_step}
\end{figure}

\subsection{Training}

We performed transfer learning from pre-trained models, rather than training
models from scratch.
Our Fast MPN-COV models were pre-trained on ImageNet 2012~\cite{ILSVRC15} and
our Faster R-CNN models were pre-trained on COCO 2017~\cite{coco}.
We used a PyTorch~\cite{pytorch} implementation of Fast MPN-COV and a
TensorFlow~\cite{tensorflow2015-whitepaper} implementation of Faster R-CNN.

\subsection{Error Correction}

Developers can train fine-grained classifiers to recognize specific mistakes
that a user of a WCA application might make when trying to complete a task.
An error state requires training data, the same way other steps of the task do.
When a frame gets classified as depicting an error state, the user is given
instructions about how to correct this.
Chapter~\ref{chap:escalation} provides further discussion about handling errors
with WCA applications.

\subsection{Development Tools}

We expanded the Ajalon tools~\cite{pham2021ajalon} to support the two stage
process described in \S\ref{sec:two_stage}.
Ajalon previously only supported a single object detector, which was sufficient
for toy examples such as the sandwich described in~\cite{chen2017}.
However, more complex assembly tasks require the use of multiple object
detectors and multiple fine-grained image classifiers.
Our improvements to Ajalon allow developers to have the application use
different computer vision models as a user progresses through a task.
This results in a single application that will automatically start
giving users instructions for the next sub-assembly after they have completed
the previous one.

\section{Our Applications}

To validate our approach, we developed WCA applications for three real assembly
tasks.
We trained models for these applications using real videos that were recorded
using a smartphone.
The videos were manually annotated with bounding boxes using Intel's Computer
Vision Annotation Tool (CVAT).
We cleaned up our dataset by computing the perceptual hash values of every
image.
For all sets of images with identical perceptual hash values, we removed all but
one of the images.
This resulted in a set of images that all had unique perceptual hash values.
We have integrated CVAT and code to remove frames with identical perceptual hash
vales into the Ajalon toolchain.

We have posted\footnote{\url{https://cmusatyalab.github.io/roger-thesis/}} all
of the
artifacts required to run these applications, along with videos showing them
being used.

\subsection{Stirling Engine}\label{sec:stirling}

The Stirling Engine WCA application guides users through disassembling a
Stirling engine.
Figure~\ref{fig:stirling_full} depicts the fully-assembled Stirling Engine.
This task requires 22 steps. All of the parts are made out of
metal, with the exception of one ring that is made out of silicone. Some steps
just require removing a single screw, and the engine looks very similar before
and after these steps have been completed.
We split the task into four subassemblies, which are shown in
Figure~\ref{fig:stirling_subs}.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/stirling_subassemblies.pdf}
  \caption{
    The steps detected by our Stirling Engine WCA application.
    The blue rectangles represent subassemblies.
    Each subassembly corresponds to a different Fast MPN-COV model.
  }\label{fig:stirling_subs}
\end{figure}

Several steps of the task involved removing screws from the engine.
The labels for these steps indicated the number of screws visible in the frame,
rather than being unique to the specific step of the task.
For example, in Figure~\ref{fig:stirling_steps}, the first and third steps
were both given the label ``2 Black Screws.''
The training script for Fast MPN-COV randomly flips images
horizontally, so we did not want the label to depend on the orientation of
objects.
The initial steps for this task all require removing screws or flipping the
engine to show screws that were previously occluded.
Therefore, every step changes the number of screws that are visible.
Designing the workflow this way made the computer vision tractable.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/stirling_steps.pdf}
  \caption{Four states from our WCA application for a Stirling engine. The
    steps look visually similar aside from the number of screws that are
    visible. The text in colored boxes are the labels that our image classifier
    was trained on.
    Note that some different steps were given the same label, but consecutive
    steps must have different labels.
    The text in the white boxes describes the actions users take to complete a
    step.
  }\label{fig:stirling_steps}
\end{figure}

We found that illuminating the engine with a table lamp increased the accuracy
of the application beyond what we could achieve with overhead room lighting.
We lit the object the same way when capturing training data and using the
application.

We created a second version of the Stirling Engine application to guide users
through assembling the engine.
This version used the exact same computer vision models as our application for
disassembly.
However, the application requires the labels to be observed in the opposite
order, because the order of steps is reversed.

\subsection{Ikea Cart}\label{sec:ikea_cart}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/full_cart.jpg}
  \caption{
    The fully assembled utility cart.
  }\label{fig:full_cart}
\end{figure}

Our next application provides guidance for users assembling an Ikea Raskog
utility cart.
The fully assembled cart is depicted in Figure~\ref{fig:full_cart}.
The task requires twenty steps to complete successfully.
However, the cart has two pairs of identical components that must be
assembled the same way.
Therefore, four of the steps are repeats of earlier steps.
The application uses the same label in cases where steps are identical.
Thus there were 16 labels, that each corresponded to the 16 unique steps.
In addition, we developed the application to detect one error state, so there
were 17 possible labels that our models could output.
We split the task into three subassemblies, which are shown in
Figure~\ref{fig:ikea_cart}.

The repeated steps are repeated in pairs. For example, step 1 is performed,
followed by step 2.
Then both are repeated.
Repeating steps in pairs avoids the situation where two consecutive steps
correspond to the same label from the classifier.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/ikea_subassemblies.pdf}
  \caption{
    The steps detected by our Ikea WCA application.
    The blue rectangles with rounded corners represent subassemblies.
    The sequences of steps in white rectangles ([A-1, A-2] and [A-3, A-4])
    are repeated.
    The error state (B-4) appears in the red circle.
  }\label{fig:ikea_cart}
\end{figure}

\subsection{Toy Car}

\begin{figure}
  \includegraphics{figures/full_toycar.jpg}
  \caption{
    The fully assembled model car.
  }\label{fig:full_toycar}
\end{figure}

The last application guides users through assembling a model car.
The fully assembled model car is shown in Figure~\ref{fig:full_toycar}.
This task requires 28 steps, which we split into 6 subassemblies.
These steps and subassemblies are shown in Figure~\ref{fig:toy_car}.
The computer vision models output a unique label for each step of the task.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/toycar_subassemblies.pdf}
  \caption{
    The steps detected by our Toy Car WCA application.
    The blue rectangles represent subassemblies.
  }\label{fig:toy_car}
\end{figure}

\section{Implementation Details}\label{sec:implementation_details}

We captured images at 1920x1080 pixels, and transmitted these to the cloudlet at
their full resolution.
This is the highest resolution that Android CameraX's ImageAnalysis use case
supports.
After processing the images with Faster R-CNN, the application crops them around
the region that likely contains the object.
The cropped image is resized to 448x448 pixels and then classified by Fast
MPN-COV.
By starting with a large initial image, we ensured that the cropped image would
be at least 448x448 pixels.
Figure~\ref{fig:crop_examples} shows examples of images before and after being
cropped.

\begin{figure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=7cm]{figures/two_stage/car_original.jpg}
    \includegraphics[width=3cm]{figures/two_stage/car_crop.jpg}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=7cm]{figures/two_stage/cart_original.jpg}
    \includegraphics[width=2cm]{figures/two_stage/cart_crop.jpg}
  \end{subfigure}
  \caption{
    Images of the toy car and Ikea cart kits, before and after being cropped.
  }\label{fig:crop_examples}
\end{figure}

The code for many computer vision models is written to run inference on batches
of images that are stored on disk. The torchvision package contains functions
for loading images from disk, in batches. Using these models in WCA applications
requires modifying the code to run the models on images being transmitted over
the network, one by one. The input batch size must be set to 1, because anything
larger would require building up a queue of images that would be run through the
model together as a single input. A larger batch size would improve the
frame rate for inference, but hurt the latency.

Live data must be as similar as possible to the data that the models are trained
on. For example, converting a
JPEG image to raw pixel values using OpenCV will result in slightly different
values than using Pillow will. We observed that our Fast MPN-COV model performed
significantly better with images opened using Pillow than with images opened
using OpenCV. The training images were opened using Pillow, but we did not
expect opening JPEGs with OpenCV and Pillow to result in different color values.

Processing images while a user is in the middle of a step wastes bandwidth and
computing resources on cloudlets.
In addition, it might lead to an application mistakenly
believing that a step has been completed before it actually has been.
We did not train our models on any images of the assemblies in partially
completed states.
We experimented with a filter that will only run images through the DNNs when
a certain number of consecutive frames have identical perceptual hash values.
This essentially means that a certain number of images in a row all had to look
very similar.
This will only occur when there is no motion in the frame, and the camera is not
moving.
Figure~\ref{fig:perceptual_hash} shows examples of images with different and
identical perceptual hash values.
Requiring a sequence of images to look similar reduced the number of frames that
had to be processed using DNNs, and it helped avoid cases where the application
erroneously detected completed steps.
This technique worked well for WCA applications running on a smartphone mounted
to a tripod.
But it was less effective for WCA applications running on a Google Glass, due to
the motion of the user's head.
Instead, we required a sequence of sequential images to be assigned the same
label by the classifier.
This helped avoid cases where the application mistakenly believed a step had
been completed while a user was still in the middle of it.
But it did not save computing resources on the cloudlet, because every frame had
to be processed.

\begin{figure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=5cm]{figures/perceptual_hash/identical1.jpg}
    \includegraphics[width=5cm]{figures/perceptual_hash/identical2.jpg}
    \caption{Two Images with identical perceptual hash values
    }\label{fig:perceptual_hash_identical}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics[width=5cm]{figures/perceptual_hash/identical2.jpg}
    \caption{An image with a perceptual hash value that is different from the
    images in Figure~\ref{fig:perceptual_hash_identical}}
  \end{subfigure}
  \caption{Images with identical and different perceptual hash values.
  }\label{fig:perceptual_hash}
\end{figure}

\section{Guidelines for WCA Developers}

We spent a significant amount of time planning out our applications before we
began any development work.
The first part of planning involved establishing what each step of the task
should be.
In particular, a developer should take pictures of what each completed step
should look like and write the text for each instruction.

\subsection{Subassemblies}

Steps that begin with showing a new part to the camera should be the start of a
new subassembly. For example, steps A-1, B-1, C-1, and D-1 in
Figure~\ref{fig:toy_car} all start with a new part.
Steps that involve rotating or repositioning the object being assembled were
sometimes chosen as the start of a new subassembly.
Steps B-1 and B-4 in Figure~\ref{fig:stirling_subs} both involve rotating or
repositioning, but we chose to make step B-1 the start of a new subassembly
while we did not make step B-4 the start of a new subassembly.
None of our applications used a step that involved adding a part to the object
being assembled as the start of a new subassembly.

\subsection{Training Data}

It is important to collect training data for each step of the task using a
variety of camera angles, backgrounds, and lighting conditions.
After training a model, test it on new data that was not included in the
training or validation sets.
If the performance of the model is not acceptable, create a new training
set that includes test images that were classified incorrectly.
Then train a new model using this set.
The new model should then be evaluated on a different fresh test set that has
not been used for anything.
