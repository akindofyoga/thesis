\chapter{Introduction}\label{chap:intro}

Wearable Cognitive Assistance (WCA) applications provide guidance to users for
a specific task.
This task could range from assembling a physical object, remembering people's
names, exercising, or playing a game or sport.
These applications
utilize mobile devices, such as smart glasses or a smartphone, to capture data
and interact with the user.
WCA applications process captured data in order to determine the physical state
of the task, and then provide assistance based on the task's physical state.

Table~\ref{table:existing_wca} lists some
examples of WCA applications that have been developed.
All of these applications determine a task's physical state based on images from
an RGB camera.
This cameras may be mounted on glasses that the user wears, or held in a tripod
with a view of the user's workspace.
Feedback is provided in the form of synthesized speech and images shown on the
display of the mobile device.

WCA is a compelling use case for edge computing. Many of these applications
utilize large deep neural network (DNN) models that are too computationally
intensive to run on a small and lightweight mobile device. However, these
applications generate large volumes of data that must be processed quickly.
Therefore, computation must be offloaded to a server with close network
proximity to the mobile device that is capturing the data~\cite{satya14}. We
will henceforth refer to this server as a cloudlet.

This work focuses on WCA applications that help users complete physical
assembly tasks.
Users are given step by step instructions, which requires the application to
determine when a user has completed a step of the task.
The application accomplishes this by processing frames from an RGB camera.
Applications have been
developed for a lego kit~\cite{lego}, a lamp~\cite{lamp}, and a toy
sandwich~\cite{sandwich}.
These tasks all required fewer than ten steps and
used parts that had distinct shapes and colors.
Taking WCA applications to the next level will require supporting tasks with
many more parts, many more steps, parts that are small relative to the full
object being assembled, and a combinatorial explosion of error states.
We propose to address these challenges in this research.

One significant challenge is the amount of labeled data that is required for
training computer vision models. Each
step of the task, and every error state that the developer would like to detect,
must be represented in the data that the models get trained on.
Increasing the number of steps thus directly increases the amount of data that
is required for training the models.
Collecting and labeling all of this data is an incredibly time-consuming
process.
This process can take up to 2 or 3 hours per task step.
Developing these models is an iterative process, which involves testing under
different lighting conditions, and then collecting more data in environments
that the model performs poorly in.
Creating training sets is thus a significant barrier to developing WCA
applications that support large numbers of steps and large numbers of parts.

Assembly tasks might involve parts that are much smaller than the full object
being assembled.
For example, one step might require the user to insert a screw into a large
metal piece.
The application needs to be able to detect when steps involving small screws
have been completed, as well as steps involving larger parts.
In addition, there are many possible ways that a user can complete a task step
incorrectly.
In fact, the number of possible errors that a person can make while completing a
task is significantly larger than the number of steps that are required to
complete the task.

\section{Thesis Statement}

\textbf{
  Scaling up WCA to complex assembly tasks is challenging because of
  (a) the difficulty of
  vision-based state detection with very small parts in the context of much
  larger objects being assembled; (b) the combinatorial explosion
  of possible error states; and (c) the large manual effort needed to create
  accurate DNNs that can reliably determine when task steps have been completed.
  These problems can be solved by a combination of (1) hierarchical
  decomposition of
  complex assemblies into modular compositions of subassemblies, (2) on-demand
  seamless
  escalation for live expert assistance, and (3) synthetic generation of
  training
  sets for born-digital components. The resulting solution can be implemented in
  a scalable and maintainable way using modular software components.
  This will enable the development of WCA applications for more complex tasks,
  which is a necessary step along the path towards making WCA applications
  practical for real world tasks.
}

\section{Potential Impact}

The following issues currently make WCA applications impractical for real world
assembly tasks:
\begin{itemize}
\item Current techniques do not support tasks with more than ten steps, and the
  steps must have significant visible differences from each other.
\item A developer must collect and label training data for each error state
  that the application detects.
  However, there are exponentially more error states than there are correct
  states, for any given task.
\item The computer vision models that we use for WCA applications require
  thousands of images depicting each state.
  All of these images must be labeled with a bounding box.
  Collecting and labeling these images requires an immense manual effort.
\end{itemize}

This thesis addresses all three of these issues.
We present new techniques that

\section{Novelty}

\section{Roadmap}
