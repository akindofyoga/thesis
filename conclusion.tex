\chapter{Conclusion and Future Work}\label{chap:conclusion}

This chapter concludes the dissertation by summarizing our contributions and
discussing open problems and future work that will further our goal of making
WCA applications practical.

\section{Contributions}

We address the problem of \textit{Scaling Up Wearable Cognitive Assistance for
  Assembly Tasks} that involve many parts.
The thesis we validated is:

\textbf{
  Scaling up WCA to complex assembly tasks is challenging because of
  (a) the difficulty of
  vision-based state detection with very small parts in the context of much
  larger objects being assembled; (b) the combinatorial explosion
  of possible error states; and (c) the large manual effort needed to create
  accurate DNNs that can reliably determine when task steps have been completed.
  These problems can be solved by a combination of (1) hierarchical
  decomposition of
  complex assemblies into modular compositions of subassemblies, (2) on-demand
  seamless
  escalation for live expert assistance, and (3) synthetic generation of
  training
  sets for born-digital components. The resulting solution can be implemented in
  a scalable and maintainable way using modular software components.
  This will enable the development of WCA applications for more complex tasks,
  which is a necessary step along the path towards making WCA applications
  practical for real world tasks.
}

We propose computer vision techniques that make it possible to detect when steps
of these long tasks have been completed.
We also demonstrate the feasibility of training computer vision models for WCA
using synthetic images.
To handle the combinatorial explosion in the number of errors states for a task,
we support escalation to a human task expert, and we create tools to determine
the number of experts required to keep queuing times reasonable.
We then redesign and re-implement the Gabriel software framework for WCA
applications, to improve scalability.
These contributions mark an important step towards making WCA applications
practical for real world assembly tasks.
Developers need a way to support real world assembly tasks, without spending an
unreasonable amount of time collecting and labeling training images.
WCA applications need some way to guide users who have made errors completing
tasks.
Lastly, WCA deployments must scale to multiple users.

\section{Future Work}

\subsection{Subassembly Identification}

WCA application developers need to split assembly tasks up into subassemblies.
This is presently a manual process, but splitting tasks up automatically would
reduce the burden on the developer.
\citet{subassembly_identification} automatically identified subassemblies of
assembled objects based on CAD files.
However, they were just concerned with making it as easy as possible for people
to assemble the objects.
We are also need to detect completed task steps using computer vision.
There are many ways to split an object up into different subassemblies.
The split that results in the easiest assembly process might make the computer
vision task difficult.
Subassembly identification for WCA requires making the assembly process easy,
and making the computer vision problem tractable.
On top of this, WCA developers might not have access to the CAD model for a kit.
WCA developers need a way to split objects into subassemblies without using
a CAD model.

\subsection{Detecting Environmental Issues}

Our computer vision models might output an incorrect label if the object being
assembled is positioned at an awkward angle, or if the lighting in a room is
too dim.
WCA applications have no way of detecting when one of these issues has
occurred.
However, if the applications could detect issues like this, they could alert the
user.
The user could then correct the issue, and avoid the incorrect computer vision
results that the issue would have caused.
For example, the application could provide audio guidance to the user that
says ``hold this part differently, or at a different angle.''

Practitioners running WCA applications can record traces of real users
completing a task with the help of a WCA application.
This will allow the practitioners to collect images with objects at an angle
that causes the application's models to output an incorrect result.
The practitioners can then train the model to recognize an error state that
corresponds to an instruction to correct the issue.

\subsection{Computer Vision Techniques}

Deep Neural Networks perform best when the training, validation, and testing
data are drawn from the same distribution as the data that the trained model
will be used on~\cite{pmlr-v97-recht19a}.
Unfortunately, this will rarely be the case in practice for WCA applications.
The lighting conditions that the application is used in and defects in how parts
are manufactured can introduce biases in the data that the application will see
at runtime.
Biases that did not exist in the training data can cause the DNNs used by the
application to perform poorly.
Each task that a developer creates a WCA application for requires its own
training set.
This limits the size of the training set that would be practical to collect in
order to develop one of these applications.
Computer vision models that are robust to biases that did not exist in the
original training set, or models that can be refined at runtime to address
biases, will improve the reliability of WCA applications.

The computer vision models used by our applications are trained to identify the
completed step shown in an image.
But they are not trained on any data depicting incomplete steps.
Our applications currently avoid telling a user that a step has been completed,
while the user is actually in the middle of the step, using the techniques
described in \S\ref{sec:implementation_details}.
In addition, users hands often cover up parts of an assembly while they
are completing steps of a task, which prevents the object detector from finding
the subassembly in an image.
However, additional work is needed to reliably prevent the application from
giving the next instruction while the user is still in the middle of the
previous step of the task.

\subsection{Textures for 3D Models}

CAD designs for born-digital objects specify the shapes of parts.
However, they rarely include information about the materials that objects are
manufactured from.
In fact, a single CAD design can be used to manufacture objects out of multiple
different materials.
A person who wants to generate synthetic data for a new born-digital object must
specify texture information for the material that the object was made out of.
This is a time consuming process.
In particular, the person must ensure that the object looks realistic in a
variety of simulated lighting conditions.
Reducing the manual effort required to specify texture information will make it
easier to generate synthetic training images.

\subsection{Device and Cloudlet Implementations}

Wearable devices that are more powerful than the ones used in our experiments
are likely to be developed.
With any mobile device, there is a tension between the device's size and
weight, its computing capability, and its battery life.
As mobile devices improve, devices without such constraints are likely to
improve more~\cite{Satya2021}.
For this reason, we forsee that there will continue to be a gap between the
accuracy of models that can be run directly on mobile devices, and models that
can be run on a cloudlet.
WCA applications will thus still require edge computing in order to achieve the
highest possible accuracy for computer vision models.
However, these applications will benefit from improvements to future smart
glasses.

In addition to hardware improvements, new computer vision techniques will be
developed that will further push the boundaries on computer vision processing
that can take place on mobile devices.
WCA applications can leverage these improvements through thin clients and split
computing approaches similar to those detailed in
Chapter~\ref{chap:implementation}.
These applications can also run local computations to avoid sending certain
frames to the cloudlet.
For example, an early discard filter might determine that there is no way that a
frame shows that the current step has been completed, so the frame will not
require any further processing.
If this filter can be run on a mobile device, the device can avoid transmitting
filtered frames to the cloudlet.

\subsection{Development Tools}

Open Workflow Editor can only be used to create applications that give the user
instructions and then process camera images until the application determines
that the user has completed a step or reached an error state.
Certain task steps might be time consuming, which makes it computationally
expensive to process frames for the entire time a user is completing a step.
Allowing developers to limit periods when images are being processed would save
on these costs.
For example, a user could press a button on the side of the headset, in order to
indicate that they believe that a step has been completed.
The application would then only have to process images after this button is
pressed.
Open Workflow Editor will have to be extended in order to support this.

Applications created with Open Workflow Editor must determine when steps have
been completed based on the output of DNNs.
However, developers might want to employ other techniques, such as length
measurement or rules-based classifiers.
In addition, Open Workflow Editor only supports applications that process images
from an RGB camera.
Allowing developers to create applications with Open Workflow Editor that
utilize techniques that are not DNNs and sensors that are not RGB cameras will
require a substantial software engineering effort.
Almost every WCA application that has been developed for assembly tasks
exclusively utilizes RGB cameras and DNNs for image processing.
Any developer considering adding a new feature to Open Workflow Editor should
first develop new WCA applications that use this feature, without using Open
Workflow Editor.
This will establish the value of such a feature, and help the
developer understand how it should be implemented.
This experience will also inform the user interface that Open Workflow Editor
should have developers use to set up this feature in new applications.

The tools used for synthetic training image generation require a CAD design file
for the object being assembled.
Other strategies must be employed to make it feasible for developers to create
WCA applications for kits that the developers do not have CAD designs for.
These strategies might involve generating synthetic images based on 3D scans,
data augmentation based on photographs of the kit, or training computer vision
models using few-shot learning~\cite{fewshot}.

\subsection{Multi-Modal Sensing}

This dissertation examines WCA applications that determine when task steps have
been completed by processing images from an RGB camera.
Collecting data from torque sensors and force sensors would increase what
applications can detect, beyond what is possible with RGB cameras alone.
For example, an application could determine if a user tightened a bolt too
tightly, or if a user did not attach two parts together with enough force.
Sensors beyond RGB cameras might be of particular use for the assembly of
objects that are extremely large.
Determining step completion for objects like these might require a user to stand
back, so that an entire object is in view of a headset's camera.
The work of \citet{sensors} represents one extreme, where an entire Ikea
wardrobe was outfitted with gyroscopes, accelerometers, force sensing resistors,
and infrared distance meters in order to determine step completion.
However, multi-modal sensing with a mix of these sensors and an RGB camera might
represent the best of both worlds.
An RGB camera can be used where possible, to avoid having to install sensors on
the kit being assembled.
But torque sensors and force sensors can be used for steps whose completion
cannot be  sufficiently detected using RGB cameras.
