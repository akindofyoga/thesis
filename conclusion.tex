\chapter{Conclusion and Future Work}\label{chap:conclusion}

\section{Contributions}

\section{Future Work}

\subsection{Detecting Environmental Issues}

Our computer vision models might output an incorrect label if the object being
assembled is positioned at an awkward angle, or if the lighting in a room is
too dim.
WCA applications have no way of detecting when one of these issues have
occurred.
However, if the applications could detect issues like this, it could alert the
user.
The user could then correct the issue, and avoid the incorrect computer vision
results that the issue would have caused.

\subsection{Computer Vision Techniques}

Deep Neural Networks make the assumption that training and testing data are
drawn from the same distribution.
Unfortunately, this will rarely be the case in practice for WCA applications.
The lighting conditions that the application is used in and defects in how parts
are manufactured can introduce biases in the data that the application sees at
runtime.
Biases that did not exist in the training data can cause the DNNs used by the
application to perform poorly.
Each task that a developer creates a WCA application for requires its own
training set.
This limits the size of the training set that would be practical to collect in
order to develop one of these applications.
If computer vision models that are more robust to differences in training and
test datasets are developed, this will improve the reliability of WCA
applications.

\subsection{Textures for 3D Models}

CAD designs for born-digital objects specify the shapes of parts.
However, they rarely include information about the materials that objects are
manufactured from.
In fact, a single CAD design can be used to manufacture objects out of multiple
different materials.
A person who wants to generate synthetic data for a new born-digital object must
specify texture information for the material that the object was made out of.
This is a time consuming process.
In particular, the person must ensure that the object looks realistic in a
variety of simulated lighting conditions.
Reducing the manual effort required to specify texture information will make it
easier to generate synthetic training images.